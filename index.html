<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Label Delay in Continual Learning</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">

    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <!-- <script type="text/javascript" async
        src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script> -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <style>
        #viz {
            border: 2px solid black;
        }
        #primary-content {
            margin: 0 auto;
            max-width: 1000px;
            padding: 0px;
        }
        .image-container {
            overflow: hidden;
            position: relative;
        }
        .image-container img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        .grayscale {
            filter: url(#grayscale);
            filter: grayscale(100%);
        }
        .button-rect {
            fill: #ccc;
        }

        .button-rect:hover {
            fill: #aaa; /* Darker shade for hover effect */
            cursor: pointer;
        }

        .button-text {
            pointer-events: none; /* This makes sure text doesn't interfere with the hover effect */
        }

        .main-fig{
            width: 85%;
        }
        .fig {
            display: flex;
            align-items: center;
            justify-content: center;
            width: 100%; /* Full width of the parent */
        }

        .fig > svg {
            width: 70%; /* SVG takes 70% of its container */
        }
        .bottom-dashed-border {
           border-bottom: 2px dashed #000;  /* 2px width, dashed style, and black color */
        }
        .top-dashed-border {
           border-top: 2px dashed #000;  /* 2px width, dashed style, and black color */
        }
        body {
            font-family: 'EB Garamond', serif;
            font-size: 22px;
            text-align: justify;
        }

        section {
            margin: 0 auto;
            max-width: 1000px;
            padding-top: 20px;
            text-align: justify;
        }

        svg {
            border: 2px solid gray;
        }

        a {
            color: black;
        }
        
    </style>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TCWJ0JB5KY"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TCWJ0JB5KY');
</script>

<body>
    <div id="primary-content">
        <center>
            <h1>Label Delay in Continual Learning</h1>
            <h4>
                <a href="https://botcs.github.io/academic/">Botos Csaba</a><sup>12</sup>*,  
                <a href="https://cemse.kaust.edu.sa/vcc/people/person/wenxuan-zhang">Wenxuan Zhang</a><sup>3</sup>*, 
                <a href="https://matthias.pw/">Matthias Müller</a><sup>2</sup>, 
                <a href="https://sites.google.com/site/sernam">Ser-Nam Lim</a><sup>4</sup>,  
                <a href="https://cemse.kaust.edu.sa/vcc/people/person/mohamed-elhoseiny">Mohamed Elhoseiny</a><sup>3</sup>,  
                <a href="https://www.robots.ox.ac.uk/~phst/">Philip Torr</a><sup>1</sup>,  
                <a href="https://www.adelbibi.com/">Adel Bibi</a><sup>1</sup></h4>
            <h5><sup>1</sup>University of Oxford, <sup>2</sup>Intel, <sup>3</sup>KAUST, <sup>4</sup>Facebook AI Research, * Equal contribution</h5>

            <h5>
                [<a href="https://arxiv.org/abs/2312.00923">Paper</a>]
                [<a href="https://github.com/botcs/label-delay-cl">Code</a> (coming soon)]
                [<a href="./demo.html">Demo</a>]
            </h5>
            <div id="fig1" class="main-fig">
                <!-- <svg width="1000" height="500" id="fig1-svg"> -->
                <svg id="fig1-svg" viewBox="0 0 1000 500">
                    <defs id="fig1-defs">
                        <filter id="grayscale">
                            <feColorMatrix id="matrix" type="saturate" values="0"></feColorMatrix>
                        </filter>
                        <marker
                            id="arrow"
                            markerUnits="strokeWidth"
                            markerWidth="12"
                            markerHeight="12"
                            viewBox="0 0 12 12"
                            refX="6"
                            refY="6"
                            orient="auto">
                            <path d="M2,2 L10,6 L2,10 L6,6 L2,2" style="fill: rgb(0, 0, 0);"></path>
                        </marker>
                    </defs>
                </svg>
                <div style="text-align:justify; margin-top:20px">
                Our proposed Continual Learning setting considering <b>Label Delay</b> allows us to model a wide range of real-world applications where new raw data is revealed significantly sooner by the data stream $\mathcal{S}_{\mathcal{X}}$ than the annotation process $\mathcal{S}_{\mathcal{Y}}$ can provide the corresponding labels. The main objective is to maximize the accuracy on the newest <b>Eval</b> data using both the samples that have already received their label (in colour) and the more recent samples that are yet to be labeled (in gray).
                </div>
                <!-- The goal is to learn a model that can perform well on a sequence of tasks, where the labels for each task are delayed by a fixed number of steps. We propose a simple yet effective approach to model label delay, which we call <b>Delayed Labels</b>. We show that our approach can be used to model a wide range of label delay distributions, and that it can be easily integrated into existing continual learning methods. We evaluate our approach on a variety of continual learning benchmarks, and show that it consistently outperforms existing methods. -->
            </div>
            
            <section>
                <h2 style="text-align:left;">Where does label delay come from?</h2>
                In many real world scenario, the time between making predictions and the feedback can be vastly different due to the inherent nature of the task.
                Consider the following three examples:
                In medical applications, the predicted post-operation recovery time of the patient is one of the most important metrics, yet the official recovery time is only established during follow-up visits.
                In investment banking, the time it takes to receive the results of a trade can be significantly longer than the time it takes to execute the trade itself.
                In the world of copyright claims, an automated trigger mechanism can prevent fraudulent usage of the content sharing platform, however the actual evaluation of each case by the owners is often significantly delayed.
            <div id="fig2" class="fig" style="text-align:center; margin-top:20px">
                <svg id="fig2-svg" viewBox="0 0 700 280">
                    <defs id="fig2-defs">
                        <filter id="grayscale">
                            <feColorMatrix id="matrix" type="saturate" values="0"></feColorMatrix>
                        </filter>
                        <marker
                            id="arrow"
                            markerUnits="strokeWidth"
                            markerWidth="12"
                            markerHeight="12"
                            viewBox="0 0 12 12"
                            refX="6"
                            refY="6"
                            orient="auto">
                            <path d="M2,2 L10,6 L2,10 L6,6 L2,2" style="fill: rgb(0, 0, 0);"></path>
                        </marker>
                    </defs>
                </svg>
            </div>
            <div id="fig3" class="fig">
                <svg id="fig3-svg" viewBox="0 0 700 280">
                    <defs id="fig3-defs">
                        <filter id="grayscale">
                            <feColorMatrix id="matrix" type="saturate" values="0"></feColorMatrix>
                        </filter>
                        <marker
                            id="arrow"
                            markerUnits="strokeWidth"
                            markerWidth="12"
                            markerHeight="12"
                            viewBox="0 0 12 12"
                            refX="6"
                            refY="6"
                            orient="auto">
                            <path d="M2,2 L10,6 L2,10 L6,6 L2,2" style="fill: rgb(0, 0, 0);"></path>
                        </marker>
                    </defs>
                </svg>
            </div>
            <div id="fig4" class="fig">
                <svg id="fig4-svg" viewBox="0 0 700 280">
                    <defs id="fig4-defs">
                        <filter id="grayscale">
                            <feColorMatrix id="matrix" type="saturate" values="0"></feColorMatrix>
                        </filter>
                        <marker
                            id="arrow"
                            markerUnits="strokeWidth"
                            markerWidth="12"
                            markerHeight="12"
                            viewBox="0 0 12 12"
                            refX="6"
                            refY="6"
                            orient="auto">
                            <path d="M2,2 L10,6 L2,10 L6,6 L2,2" style="fill: rgb(0, 0, 0);"></path>
                        </marker>
                    </defs>
                </svg>
            </div>
            As one can see in the above examples, although the emergent problem of label delay is present across different applications, the root cause stems from entirely different sources.
            While the real world applications are heavily impacted by the phenomenon, the diversity of the various scenarios makes it difficult to find common patterns that can be used to address the problem of label delay.
            A few challenges that one might find when trying to model label delays in real-world applications are:
            <ul style="margin-left:40px">
                <li>The data distribution is evolving over time</li>
                <li>The delay factor cannot be influenced for analysis</li>
                <li>The delay impacts the model in unknown ways</li>
            </ul>
            </section>
            <section>
                <h2 style="text-align:left;">Our proposal</h2>
                <div style="text-align:justify; margin-top:20px">
                    We propose a new Continual Learning setting, in which we show how does label delay impact the learning process.
                    We consider the naïve solution of ignoring the most recently collected data and only using the samples that have already received their label and compare it to the ideal case where the labels are immediately available for all samples.
                    We provide an extensive list of experiments (amounting to over 25k GPU hours) of trying to recover the performance of the ideal case by using the samples before their corresponding labels become available.

                    <div style="margin-top:20px; width:100%; text-align: center;">
                        <img src="fig-delay-ablation-CLOC.svg" style="width:35%; margin-top:20px; display: inline-block; vertical-align: middle;">
                        <img src="fig-delay-ablation-CGLM.svg" style="width:35%; margin-top:20px; display: inline-block; vertical-align: middle;">
                    </div>

                    We use two large-scale datasets to evaluate our approach: <b>Continual Localization (CLOC - 40M samples)</b> and <b>Continual Google Landmarks (CGLM - 0.5M samples)</b>.
                    As one can see in the above figures, there is a growing gap between the performance of the ideal case and the naïve solution as the delay increases.
                    More importantly, we show that on different datasets the impact of the delay differs significantly, which highlights the importance of modeling label delay.
                </div>
            </section>
            <section>
                <h2 style="text-align:left;">How to overcome label delay?</h2>
                Even though one might not be able to influence the delay factor, we show that it is possible to recover the performance of the ideal case by using the samples before their corresponding labels become available.
                There are two main challenges that one needs to overcome in order to achieve this:
                <!-- <ul style="margin-left:40px">
                    <li>How to use the unlabeled samples to improve the model?</li>
                    <li>How to keep the solution computationally efficient?</li>
                </ul> -->
                1) using the unlabeled samples to improve the model
                2) keeping the solution computationally efficient
                To address these challenges our experiment allows the continual learning models to use the unlabeled samples, while normalizing the computational cost of the model to be the same as the naïve solution.
                <div id="fig5" class="fig" style="margin-top:20px; margin-bottom:20px;">
                    <svg id="fig5-svg" viewBox="0 0 700 400">
                        <defs id="fig5-defs">
                            <marker
                                id="arrow"
                                markerUnits="strokeWidth"
                                markerWidth="12"
                                markerHeight="12"
                                viewBox="0 0 12 12"
                                refX="6"
                                refY="6"
                                orient="auto">
                                <path d="M2,2 L10,6 L2,10 L6,6 L2,2" style="fill: rgb(0, 0, 0);"></path>
                            </marker>
                        </defs>
                    </svg>
                </div>
                This allows us to compare the performance of various methods that extend the naïve solution to use the unlabeled samples.
                Under such a setting, we can use the naïve solution as the lower bound and the ideal case as the upper bound, while keeping the computational cost $\mathcal{C}$ of the model the same.
                In our work, we compare the performance of various families of methods, such as Test-Time Adaptation (TTA) and Self-Supervised Learning (SSL) and propose a new, more efficient approach fine-tailored to the problem of label delay.

            <div style="margin-top:20px; width:100%; text-align: center;">
                <img src="fig-unsupervised.svg" style="width:85%; margin-top:20px; display: inline-block; vertical-align: middle; horizontal-align: middle;">
            </div>
            </section>

            <section>
                <h2 style="text-align:left;">Future work</h2>
                In this project, we have demonstrated the versatility of our proposed setting in modeling various label delay scenarios. 
                A key presumption in our methodology is that the rate at which data is collected is <i>identical</i> to the rate at which labels are assigned. However, this assumption doesn't always hold true in practical situations. By allowing the rates of data collection and label assignment to be modeled independently, our method could be adapted for a broader array of applications where these two rates are not identical. 
                Although our current model anticipates that each data sample will be assigned a label after a specific number of steps (exactly $d$ steps), this may not be feasible in real-world conditions where data accumulates faster than labels can be assigned, potentially leaving some samples unlabeled indefinitely.
                In such cases, the choice which samples are labeled and which are not is not arbitrary, but rather a strategic decision that can have a significant impact on the performance of the model.
                This is especially true in the case of continual learning, where the model is expected to perform well on the most recent data.
                <div id="fig6" class="main-fig" style="margin-top:20px; margin-bottom:20px">
                    <!-- <svg width="1000" height="420" id="fig6-svg"> -->
                    <svg id="fig6-svg" viewBox="0 0 1000 420">
                        <defs id="fig6-defs">
                            <marker
                                id="arrow"
                                markerUnits="strokeWidth"
                                markerWidth="12"
                                markerHeight="12"
                                viewBox="0 0 12 12"
                                refX="6"
                                refY="6"
                                orient="auto">
                                <path d="M2,2 L10,6 L2,10 L6,6 L2,2" style="fill: rgb(0, 0, 0);"></path>
                            </marker>
                        </defs>
                    </svg>
                </div>
            
            In the figure above, we implement the simplest possible strategy, where the samples are labeled in the order they are collected.
            We denote each annotator with their corresponding ID, therefore $\#1,\#2,\#3,...$ are the first, second, third annotators, respectively.
            In this example each annotator takes $d=4$ steps, and can only start labeling the next samples once they have finished labeling their assigned ones.
            If the ratio between the rate of data collection and the rate of label assignment is $r=1$, then after $d$ steps every sample will have received its label.
            However, if the ratio is $r>1$, then it means that the annotators cannot keep up with the rate of data collection and some samples will remain unlabeled indefinitely.
            In this case, the choice of which samples are labeled and which are not is a strategic decision that can have a significant impact on the performance of the model.
            <h3>How to interact with the figure:</h3>
            <ul style="margin-left:40px">
                <li><b>Data collection rate:</b> controls how fast samples are revealed by the stream</li>
                <li><b>Annotation rate:</b> controls the annotation throughput</li>
            </section>
        </center>
    </div>

    <script src="fig1.js"></script>
    <script src="fig2.js"></script>
    <script src="fig3.js"></script>
    <script src="fig4.js"></script>
    <script src="fig5.js"></script>
    <script src="fig6.js"></script>




</body>

</html>
